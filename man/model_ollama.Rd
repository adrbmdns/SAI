% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/models.R
\name{model_ollama}
\alias{model_ollama}
\title{Set the Ollama model}
\usage{
model_ollama(
  model = "llama3.1:8b",
  port = 11434,
  format = c("none", "json"),
  tools = NULL,
  temperature = 0,
  stop = NA,
  seed = 0,
  top_k = 1,
  top_p = 0.1,
  num_predict = 1000,
  keep_alive = "5m",
  .check_model_availability = TRUE
)
}
\arguments{
\item{port}{The port number of the Ollama server.}

\item{format}{The format to use. Default is "none" and currently only other option is "json".}

\item{temperature}{The temperature of the model where higher temperature means higher creativity.}

\item{stop}{A list of stop sequences to use.}

\item{seed}{The seed to use for the model.}

\item{top_k}{The top k tokens to consider.}

\item{top_p}{The nucleus sampling or penalty. It limits the cumulative probability of the most likely tokens.
Higher values allow more tokens and diverse responses, and while lower values are more focused and constrained answers.}

\item{num_predict}{The number of predictions to make.}

\item{keep_alive}{The time to keep the server alive.}
}
\description{
Set the Ollama model
}
\seealso{
Other large-lanugage-models: 
\code{\link{model_mistral}()},
\code{\link{model_openai}()},
\code{\link{model_vendor}()}
}
\concept{large-lanugage-models}
